{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f197350",
   "metadata": {},
   "source": [
    "# Importing PyTorch and Torchvision\n",
    "\n",
    "In this section, we import the necessary libraries for working with tensors, neural networks, and image datasets.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c782ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a275f13a",
   "metadata": {},
   "source": [
    "# Loading the MNIST Dataset\n",
    "\n",
    "The following line loads the MNIST dataset into your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09ce6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 57.5MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.66MB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.4MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.10MB/s]\n"
     ]
    }
   ],
   "source": [
    "dataset=MNIST(root='/data',download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6342d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20033b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset=MNIST(root='/data',train=False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea2960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28>, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52fdb23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b7114",
   "metadata": {},
   "source": [
    "# Viewing an MNIST Sample Image\n",
    "\n",
    "The following code retrieves the first image-label pair from the MNIST dataset and displays it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4398ba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image , label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "152461c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=MNIST(root='/data',train=True,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7502c7ee",
   "metadata": {},
   "source": [
    "This code retrieves the first image and label from the MNIST dataset using `img_tensor, label = dataset[0]`, where `img_tensor` is the image data and `label` is the corresponding digit (0–9). The line `print(img_tensor.shape, label)` then displays the shape of the image tensor along with its label. If `ToTensor()` was used when loading the dataset, the image will have the shape `torch.Size([1, 28, 28])`, where `1` represents the grayscale channel and `28×28` is the image size; otherwise, without a tensor transform, the image may be a PIL image and `.shape` may behave differently. In summary, this line helps you inspect both the structure of the image data and its ground-truth label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05a654dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor , label=dataset[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec93ab",
   "metadata": {},
   "source": [
    "The line `img_tensor[:, 10:15, 10:15]` extracts a 5×5 patch from the MNIST image by taking all channels (`:`) and selecting rows 10–14 and columns 10–14, allowing you to inspect the raw pixel values in that small region. Since MNIST tensors (after `ToTensor()`) contain values between 0 and 1, the printed numbers represent grayscale intensities where 0 is black, 1 is white, and anything in between is a shade of gray. The next line, `print(torch.max(img_tensor), torch.min(img_tensor))`, prints the highest and lowest pixel values in the entire image, helping you confirm the value range of the tensor—typically max = 1.0 and min = 0.0 for MNIST images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de5d9993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[:,10:15,10:15])\n",
    "print(torch.max(img_tensor),torch.min(img_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7ac5b",
   "metadata": {},
   "source": [
    "The line `plt.imshow(img_tensor[0, 10:15, 10:15], cmap='gray')` displays a small 5×5 region of the MNIST image by selecting channel 0 and slicing rows 10–14 and columns 10–14. This effectively zooms into a tiny patch of the digit, allowing you to visualize the raw pixel values. The `cmap='gray'` argument ensures the patch is shown in grayscale, where darker pixels represent lower values (closer to 0) and lighter pixels represent higher values (closer to 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa8d4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_tensor[0,10:15,10:15] , cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831726f",
   "metadata": {},
   "source": [
    "The `random_split` function is used to divide the MNIST dataset into two subsets. By calling `train_ds, val_ds = random_split(dataset, [50000, 10000])`, we randomly split the original 60,000-image dataset into a training set with 50,000 samples and a validation set with 10,000 samples. This is useful because the training set is used to fit the model, while the validation set helps evaluate performance during training to detect overfitting. The line `len(train_ds), len(val_ds)` simply returns the sizes of these two new subsets, confirming that the dataset was split correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f85b4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_ds, val_ds = random_split(dataset,[50000,10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6456c",
   "metadata": {},
   "source": [
    "This code creates PyTorch DataLoaders for the training and validation sets. After importing `DataLoader`, a batch size of 128 is chosen, meaning the model will receive 128 images at a time during training. The line `train_loader = DataLoader(train_ds, batch_size, shuffle=True)` converts the training dataset into shuffled mini-batches, which helps the model generalize by presenting the samples in a random order each epoch. The validation loader, created via `val_loader = DataLoader(val_ds, batch_size)`, also groups samples into batches but without shuffling, since validation data is not used for learning—only for monitoring performance. These loaders make it easy to iterate through data efficiently during training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7643eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "train_loader=DataLoader(train_ds , batch_size , shuffle=True)\n",
    "val_loader= DataLoader(val_ds , batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93967497",
   "metadata": {},
   "source": [
    "This code constructs a simple neural network layer for MNIST classification. After importing PyTorch’s neural-network module (`torch.nn` as `nn`), the input size is set to `28*28`, since each MNIST image consists of 784 pixels flattened into a 1D vector. The number of output classes is defined as 10, corresponding to digits 0 through 9. The line `model = nn.Linear(input_size, num_classes)` creates a fully connected (linear) layer that maps the 784-dimensional input to 10 output values. This layer performs the computation `W*x + b`, making it equivalent to a multinomial logistic regression model. It produces one score per class, which will later be passed to a loss function such as cross-entropy during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "189c84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "input_size=28*28\n",
    "num_classes=10\n",
    "\n",
    "model=nn.Linear(input_size , num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166f05c",
   "metadata": {},
   "source": [
    "The shape of `model.weight` reflects the configuration of the linear layer. Since the model is defined as `nn.Linear(784, 10)`, the weight matrix has shape `[10, 784]`, meaning there is one row of 784 weights for each of the 10 output classes. Calling `model.weight.ndim` returns `2` because the weight matrix is a 2-dimensional tensor (rows × columns). Printing `model.weight` shows the actual learnable parameters of the layer. These values are randomly initialized at the start and are updated during training as the network learns to classify the digits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be4efffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0058, -0.0070,  0.0217,  ..., -0.0188,  0.0330, -0.0313],\n",
       "        [-0.0163,  0.0299,  0.0214,  ..., -0.0013, -0.0055,  0.0102],\n",
       "        [-0.0320,  0.0333, -0.0233,  ..., -0.0157,  0.0347, -0.0075],\n",
       "        ...,\n",
       "        [ 0.0119, -0.0042,  0.0165,  ..., -0.0198, -0.0224,  0.0320],\n",
       "        [ 0.0024,  0.0301,  0.0223,  ..., -0.0286, -0.0090, -0.0011],\n",
       "        [-0.0208,  0.0050, -0.0323,  ...,  0.0175,  0.0331, -0.0330]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "print(model.weight.ndim)\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a5473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0083, -0.0266,  0.0061, -0.0270, -0.0113,  0.0232, -0.0160, -0.0142,\n",
       "        -0.0262, -0.0301], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.bias.shape)\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc891e7",
   "metadata": {},
   "source": [
    "This error occurs because the Linear layer expects the input to have a specific shape, but the images are being passed in without flattening. The model was defined as `nn.Linear(784, 10)`, which means it expects each input sample to be a 1D vector of size **784** (since 28×28 = 784). However, the batch of images arriving from the DataLoader has shape `[batch_size, 1, 28, 28]`, and when PyTorch internally tries to multiply this with the weight matrix of shape `[10, 784]`, the dimensions don't line up for matrix multiplication. The error `mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)` shows that PyTorch attempted to treat the input as a matrix with width 28 instead of 784, meaning the tensor was not flattened. Since matrix multiplication requires the inner dimensions to match, and 28 ≠ 784, PyTorch raises an error. Flattening the images to shape `[batch_size, 784]` fixes the issue and allows the Linear layer to perform the required multiplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6995b897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "367f8e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9089ce0",
   "metadata": {},
   "source": [
    "The expression `images.reshape(128, 784).shape` reshapes the batch of MNIST images from `[128, 1, 28, 28]` into `[128, 784]`. Each image originally has 1 channel and a 28×28 grid of pixels, but a linear layer (`nn.Linear`) requires the input to be a flat 1-dimensional vector. Calling `.reshape(128, 784)` takes all 28×28 = 784 pixel intensities from each image and flattens them into a single row, producing a batch where each of the 128 images is now represented as a 784-element vector. The resulting shape `(128, 784)` confirms that the batch has 128 samples and each sample has 784 features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c39a605b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.reshape(128, 784).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb0ed9c",
   "metadata": {},
   "source": [
    "This class defines a simple neural network for MNIST using PyTorch. The model inherits from `nn.Module`, allowing PyTorch to track learnable parameters. In the constructor (`__init__`), a single linear layer is created with `nn.Linear(input_size, num_classes)`, mapping each 784-element flattened MNIST image to 10 output scores. The `forward` method defines how data flows through the model: the input batch `xb`, originally shaped `[batch_size, 1, 28, 28]`, is reshaped into `[batch_size, 784]` using `xb.reshape(-1, 784)`, where `-1` tells PyTorch to infer the batch size automatically. The flattened tensor is then passed through the linear layer to produce a tensor of shape `[batch_size, 10]`, representing the predicted scores (logits) for digits 0–9. Finally, `model = MnistModel()` creates an instance of the model with initialized weights, ready for training or inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d4e0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size , num_classes)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb=xb.reshape(-1 , 784)\n",
    "        out=self.linear(xb)\n",
    "        return out\n",
    "\n",
    "model=MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9aff0517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=10, bias=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c64542",
   "metadata": {},
   "source": [
    "The output in the screenshot shows the shapes and actual values of the learnable parameters inside the model’s linear layer. The weight matrix has shape `[10, 784]` because the model must transform each flattened MNIST image (784 input features) into 10 output logits—one for each digit class. Each of the 10 rows in this matrix corresponds to the weight vector used to score a specific digit (0–9), and each row contains 784 weights, one for each pixel. The bias vector has shape `[10]` because there is one bias term added to each output class. When you call `list(model.parameters())`, PyTorch prints these two tensors: first the 10×784 weight matrix and then the length-10 bias vector. All values are small random numbers because PyTorch initializes Linear layer parameters randomly before training, and `requires_grad=True` indicates that these parameters will be updated during backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56318602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0024, -0.0213, -0.0263,  ..., -0.0122,  0.0336,  0.0098],\n",
       "         [ 0.0007,  0.0059, -0.0057,  ..., -0.0103, -0.0076, -0.0289],\n",
       "         [-0.0060, -0.0202, -0.0048,  ...,  0.0027,  0.0077,  0.0352],\n",
       "         ...,\n",
       "         [-0.0314,  0.0281,  0.0109,  ..., -0.0141, -0.0259, -0.0178],\n",
       "         [-0.0217, -0.0268,  0.0124,  ...,  0.0274,  0.0242, -0.0141],\n",
       "         [-0.0064, -0.0304,  0.0081,  ..., -0.0320, -0.0320, -0.0072]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0077, -0.0314,  0.0335, -0.0268,  0.0127, -0.0026, -0.0328,  0.0258,\n",
       "          0.0287, -0.0336], requires_grad=True)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01833d84",
   "metadata": {},
   "source": [
    "This code takes one batch of images from the DataLoader, feeds them through the model, and prints the shape and sample predictions. Inside the loop, `outputs = model(images)` performs a forward pass, converting each image in the batch into a vector of 10 logits (raw class scores). The `break` ensures that only the first batch is processed. Printing `outputs.shape` typically shows something like `[128, 10]`, meaning there are 128 samples in the batch and each sample has 10 output values—one score per MNIST digit class (0–9). The line `outputs[:2].data` prints the first two prediction vectors, each containing 10 numbers. These numbers are unnormalized logits, not probabilities; they represent the model’s raw confidence levels before applying softmax or a loss function like cross-entropy. During training, these logits will be used to compute the loss and update the model’s weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0a67d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape:  torch.Size([128, 10])\n",
      "Sample Outputs:  tensor([[-0.1910, -0.0086, -0.0243, -0.2442,  0.3778,  0.3974, -0.2094, -0.0050,\n",
      "          0.1302,  0.5409],\n",
      "        [ 0.0098, -0.3177,  0.0795,  0.0471,  0.0243,  0.1104, -0.0192, -0.1454,\n",
      "          0.1238,  0.1026]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    outputs= model(images)\n",
    "    break\n",
    "\n",
    "print('output.shape: ', outputs.shape)\n",
    "print('Sample Outputs: ', outputs[:2].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a62a9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee9d4ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1910, -0.0086, -0.0243, -0.2442,  0.3778,  0.3974, -0.2094, -0.0050,\n",
       "          0.1302,  0.5409],\n",
       "        [ 0.0098, -0.3177,  0.0795,  0.0471,  0.0243,  0.1104, -0.0192, -0.1454,\n",
       "          0.1238,  0.1026],\n",
       "        [ 0.0304, -0.1021, -0.0964,  0.0527, -0.0655, -0.0015,  0.1368,  0.0723,\n",
       "          0.0561,  0.1976],\n",
       "        [ 0.3317, -0.3046,  0.1112,  0.0356,  0.1469,  0.3362, -0.2944,  0.0335,\n",
       "          0.1218,  0.1451]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9c380d",
   "metadata": {},
   "source": [
    "This code converts the model’s raw output logits into actual probabilities and then checks that those probabilities sum to 1. The line `probs = F.softmax(outputs, dim=1)` applies the softmax function across the 10 output values for each image, turning them into normalized probabilities between 0 and 1. Softmax works by exponentiating each logit and dividing by the sum of all exponentiated logits, ensuring that each row of the output represents a valid probability distribution over the 10 MNIST classes. Printing `probs[:2].data` shows the first two probability vectors—each containing 10 numbers where higher values indicate more confidence in particular digits. Because softmax always normalizes the outputs, the line `torch.sum(probs[0]).item()` prints `1.0`, confirming that the probabilities for the first image add up exactly to 1. This is why softmax is used when we want interpretable, class-wise probabilities instead of raw logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbd7e22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Probabilities:\n",
      " tensor([[0.0739, 0.0886, 0.0872, 0.0700, 0.1304, 0.1330, 0.0725, 0.0889, 0.1018,\n",
      "         0.1535],\n",
      "        [0.1000, 0.0721, 0.1072, 0.1038, 0.1015, 0.1106, 0.0972, 0.0856, 0.1121,\n",
      "         0.1098]])\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "probs=F.softmax(outputs, dim=1)\n",
    "print(\"Sample Probabilities:\\n\", probs[:2].data)\n",
    "print(\"Sum:\", torch.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f1bd52",
   "metadata": {},
   "source": [
    "This code extracts the model’s predicted class for each image and the corresponding probability. The function `torch.max(probs, dim=1)` looks across the 10 probability values for every sample in the batch. For each row, it finds: (1) the **maximum probability** in that row, and (2) the **index** at which that maximum occurs. The index represents the predicted digit class (0–9), because each position in the probability vector corresponds to a class. Thus, `preds` becomes a tensor of predicted labels—for example, something like `[3, 8, 1, 0, ...]`—while `max_probs` contains the highest confidence values for each corresponding prediction. Printing `preds` shows which digit the model thinks each image represents, and printing `max_probs` shows how confident the model is in each prediction. Together, they form the standard output of a classification model: predicted class and its associated probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "feee755c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 8, 9, 5, 5, 9, 9, 7, 9, 4, 9, 0, 9, 9, 9, 5, 9, 4, 5, 9, 4, 5, 4, 4,\n",
      "        4, 4, 9, 2, 5, 9, 4, 2, 9, 9, 4, 9, 5, 0, 9, 4, 5, 5, 7, 2, 2, 5, 9, 8,\n",
      "        5, 5, 9, 4, 5, 9, 4, 3, 4, 5, 2, 4, 4, 9, 9, 4, 4, 2, 9, 9, 0, 9, 9, 4,\n",
      "        3, 9, 9, 0, 9, 5, 9, 4, 8, 5, 5, 4, 9, 4, 4, 9, 4, 9, 8, 9, 6, 3, 9, 5,\n",
      "        5, 4, 5, 2, 0, 9, 9, 9, 9, 5, 9, 9, 7, 4, 8, 5, 9, 5, 5, 4, 4, 9, 7, 4,\n",
      "        5, 9, 4, 5, 9, 2, 9, 9])\n",
      "tensor([0.1535, 0.1121, 0.1180, 0.1283, 0.1310, 0.1258, 0.1343, 0.1187, 0.1325,\n",
      "        0.1183, 0.1589, 0.1242, 0.1181, 0.1252, 0.1181, 0.1365, 0.1465, 0.1257,\n",
      "        0.1269, 0.1418, 0.1265, 0.1260, 0.1198, 0.1317, 0.1267, 0.1370, 0.1313,\n",
      "        0.1326, 0.1193, 0.1486, 0.1170, 0.1366, 0.1403, 0.1312, 0.1218, 0.1398,\n",
      "        0.1337, 0.1210, 0.1445, 0.1251, 0.1760, 0.1186, 0.1104, 0.1309, 0.1545,\n",
      "        0.1099, 0.1306, 0.1329, 0.1349, 0.1243, 0.1254, 0.1389, 0.1392, 0.1257,\n",
      "        0.1353, 0.1148, 0.1188, 0.1265, 0.1327, 0.1264, 0.1167, 0.1429, 0.1505,\n",
      "        0.1601, 0.1299, 0.1255, 0.1543, 0.1512, 0.1279, 0.1471, 0.1331, 0.1313,\n",
      "        0.1127, 0.1445, 0.1263, 0.1217, 0.1612, 0.1183, 0.1213, 0.1123, 0.1217,\n",
      "        0.1219, 0.1399, 0.1339, 0.1212, 0.1252, 0.1217, 0.1831, 0.1279, 0.1157,\n",
      "        0.1187, 0.1336, 0.1174, 0.1150, 0.1496, 0.1341, 0.1456, 0.1327, 0.1176,\n",
      "        0.1217, 0.1278, 0.1379, 0.1308, 0.1418, 0.1225, 0.1209, 0.1287, 0.1304,\n",
      "        0.1238, 0.1112, 0.1113, 0.1382, 0.1583, 0.1299, 0.1325, 0.1306, 0.1287,\n",
      "        0.1235, 0.1144, 0.1220, 0.1167, 0.1422, 0.1236, 0.1487, 0.1527, 0.1279,\n",
      "        0.1288, 0.1297], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs , preds =torch.max(probs, dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c271c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1910, -0.0086, -0.0243, -0.2442,  0.3778,  0.3974, -0.2094, -0.0050,\n",
       "          0.1302,  0.5409],\n",
       "        [ 0.0098, -0.3177,  0.0795,  0.0471,  0.0243,  0.1104, -0.0192, -0.1454,\n",
       "          0.1238,  0.1026]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds=torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds==labels).item()/len(preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
