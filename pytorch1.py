# -*- coding: utf-8 -*-
"""pytorch1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ng-AN-Y7ukIpNeHcKxNWBZb45ivgwF5f

**This code prints the installed version of PyTorch to verify the environment is
correctly set up and to ensure compatibility with subsequent operations that rely on specific tensor behaviors or GPU support.**
"""

import torch
print(torch.__version__)

"""**This code checks whether a CUDA-compatible GPU is available to PyTorch, and if so, confirms GPU usage by displaying the device name; otherwise, it notifies that computations will default to CPU — enabling conditional execution based on hardware capabilities for optimized performance.**"""

if torch.cuda.is_available():
  print("GPU is available")
  print(f"using GPU:{torch.cuda.get_device_name(0)}")
else:
  print("GPU is not available")

"""# Creating an Uninitialized Tensor with `torch.empty`

This code creates a 2×3 tensor (a two-dimensional array with 2 rows and 3 columns) using `torch.empty`, which allocates a block of memory for the tensor but does *not* initialize its values — meaning the tensor contains whatever random data was already in that memory location, making it suitable for performance-critical scenarios where values will be overwritten immediately, but unsafe to use without explicit assignment.
"""

a=torch.empty(2,3)

type(a)

"""# Creating a Tensor Filled with Zeros Using `torch.zeros`

This code creates a 2×3 tensor filled entirely with zeros, initializing all elements to a value of 0.0 — commonly used to set up matrices or vectors that will be updated during training, such as bias terms, masks, or placeholder outputs, ensuring a clean, predictable starting state for computations.
"""

torch.zeros(2,3)

"""# Creating a Tensor Filled with Ones Using `torch.ones`

This code creates a 2×3 tensor where every element is initialized to 1.0 — often used to initialize scaling factors, unit matrices, or as a baseline for operations like normalization, activation functions, or when constructing identity-like structures in neural network layers.
"""

torch.ones(2,3)

"""# Creating a Random Tensor Using `torch.rand`

This code creates a 2×3 tensor populated with random values drawn from a uniform distribution between 0 and 1 — commonly used to initialize weights in neural networks when a simple, unbiased random starting point is desired, ensuring that neurons begin with diverse, non-zero activations to break symmetry during training.
"""

torch.rand(2,3)

"""# Reproducible Random Tensor Initialization with `torch.manual_seed`

This code sets a fixed random seed using `torch.manual_seed(100)` to ensure that every subsequent call to `torch.rand(2,3)` generates the exact same 2×3 tensor of random values — enabling consistent and reproducible results across runs, which is essential for debugging, experimentation, and sharing model training outcomes in research or development.
"""

torch.manual_seed(100)
torch.rand(2,3)

"""# Creating a Tensor from Python List Data Using `torch.tensor`

This code constructs a 2×3 tensor by directly converting a nested Python list into a PyTorch tensor, preserving the structure and values exactly as provided — commonly used to define small, hardcoded data such as labels, sample inputs, or fixed weights during prototyping or testing, ensuring precise control over initial tensor content without randomization or initialization.
"""

torch.tensor([[1,2,3],[4,5,6]])

"""# Creating a Sequence Tensor with `torch.arange`

This code generates a 1D tensor containing evenly spaced values starting from 0 up to (but not including) 10, with a step size of 2 — producing `[0, 2, 4, 6, 8]` — commonly used to create index arrays, time steps, or uniform sampling grids for data manipulation, loss computation, or defining ranges in neural network operations without explicit listing.
"""

print("using arange->",torch.arange(0,10,2))

"""# Creating a Linearly Spaced Tensor with `torch.linspace`

This code generates a 1D tensor with 10 evenly spaced values between 0 and 10 (inclusive), creating a smooth sequence that starts at 0 and ends at 10 — commonly used to define continuous ranges for plotting, sampling activation functions, or initializing parameters that require uniform coverage over an interval in mathematical or physical modeling within neural networks.
"""

print("using linspace ->",torch.linspace(0,10,10))

"""# Creating an Identity Matrix with `torch.eye`

This code generates a 5×5 identity matrix — a square tensor with ones on the main diagonal and zeros everywhere else — commonly used in linear algebra operations such as initializing weight matrices to preserve input magnitude, regularizing optimization, or serving as a baseline for matrix inversion and transformation tasks in neural network layers.
"""

print("using eye->", torch.eye(5))

"""# Creating a Tensor Filled with a Constant Value Using `torch.full`

This code creates a 3×3 tensor where every element is set to the specified value of 5 — useful for initializing tensors with a uniform constant, such as scaling factors, bias offsets, or padding values, ensuring all elements start with the same predefined value for consistent behavior in mathematical operations or model initialization.
"""

print("using full->",torch.full((3,3),5))

x = torch.tensor([[1,2,3],[4,5,6]])
x

x.shape

"""# Creating a Tensor with the Same Shape as Another Using `torch.empty_like`

This code creates a new tensor that has the exact same shape and data type as the input tensor `x`, but fills it with uninitialized (random) values from memory — useful for efficiently allocating memory in-place when you need a placeholder tensor of matching dimensions, such as storing intermediate outputs or gradients during forward and backward passes, without copying or initializing data unnecessarily.
"""

torch.empty_like(x)

"""# Creating a Zero-initialized Tensor with the Same Shape as Another Using `torch.zeros_like`

This code creates a new tensor with the same shape and data type as the input tensor `x`, but initializes all its elements to zero — commonly used to initialize gradients, residuals, or target outputs during backpropagation or loss computation, ensuring that operations start from a clean, known state while preserving the structure of the original tensor for safe and compatible computations.
"""

torch.zeros_like(x)

"""# Creating a One-initialized Tensor with the Same Shape as Another Using `torch.ones_like`

This code creates a new tensor with the same shape and data type as the input tensor `x`, but initializes all its elements to one — useful for initializing scaling factors, unit masks, or activation multipliers during operations like normalization, attention mechanisms, or residual connections, ensuring consistent element-wise behavior while maintaining compatibility with the original tensor’s structure.
"""

torch.ones_like(x)

"""# Understanding `torch.rand_like(x)` and Why It Differs from `torch.rand()`

Unlike functions such as `torch.rand(2,3)` or `torch.zeros(3,3)`, which take explicit shape arguments to create a new tensor from scratch, `torch.rand_like(x)` requires an existing tensor `x` as input — it does *not* accept shape tuples like `(2,3)`. This is by design: `rand_like` is intended to replicate the *structure* (shape, dtype, device) of an existing tensor, not define a new one arbitrarily. If you pass a tuple or forget to define `x`, Python raises a `NameError` or `TypeError`, because the function expects a tensor object, not dimensions. To use `rand_like` correctly, always first define your reference tensor — for example, `x = torch.randn(2, 3)` — then call `torch.rand_like(x)` to generate a random tensor with identical properties, ensuring compatibility in operations like gradient initialization or layer-wise weight replacement without manual shape management.
"""

torch.rand_like(x)

"""# Querying the Data Type of a Tensor Using `x.dtype`

This code retrieves the data type (dtype) of tensor `x`, such as `torch.float32`, `torch.int64`, or `torch.bool` — a critical property that determines how values are stored and computed, ensuring numerical precision, memory efficiency, and compatibility during operations like matrix multiplication or gradient updates in neural network training.
"""

x.dtype

"""# Creating a Tensor with Explicit Data Type Using `dtype=torch.int32`

This code creates a 1D tensor from a list of floating-point values but explicitly casts them to 32-bit integers (`torch.int32`), truncating decimal parts and converting `1.0`, `2.0`, `3.0` into the integers `1`, `2`, `3` — used when memory efficiency or integer-based operations (e.g., indexing, classification labels) are required, ensuring the tensor’s internal representation matches the computational needs of the downstream task rather than defaulting to floating-point precision.
"""

torch.tensor([1.0,2.0,3.0],dtype=torch.int32)

"""# Creating a Tensor with Explicit Double-Precision Floating Point Using `dtype=torch.float64`

This code creates a 1D tensor from integer values but explicitly casts them to 64-bit floating-point numbers (`torch.float64`), preserving full numerical precision for applications requiring high accuracy—such as scientific computing, numerical stability in optimization, or interoperability with libraries that expect double-precision inputs—while maintaining the original values as `1.0`, `2.0`, and `3.0` in memory.
"""

torch.tensor([1,2,3],dtype=torch.float64)

"""# Converting Tensor Data Type In-Place with `x.to(torch.float32)`

This code converts the data type of tensor `x` to 32-bit floating-point (`torch.float32`) without changing its shape or device, enabling compatibility with models or operations that require single-precision floats—commonly used for efficient memory usage and faster computation on GPUs during neural network training, while preserving the underlying values through precise type casting.
"""

x.to(torch.float32)

"""# Using `torch.rand_like()` with Explicit `dtype` Override

This code generates a random tensor with the same shape and device as `x`, but explicitly overrides its data type to `torch.float32` — allowing precise control over numerical precision during initialization, even when the source tensor `x` has a different dtype (e.g., `float64` or `int64`), ensuring compatibility with standard deep learning operations that assume single-precision floating-point inputs while maintaining structural consistency with existing tensors.
"""

torch.rand_like(x,dtype=torch.float32)

x=torch.rand(2,2)
x

"""# Element-wise Arithmetic Operations on Tensors

This sequence demonstrates basic element-wise arithmetic operations on a PyTorch tensor `x`: adding 2, subtracting 2, multiplying by 3, dividing by 3, performing integer division by 3 after scaling by 100, and computing the modulo 2 remainder — illustrating how PyTorch broadcasts scalar values across all tensor elements for efficient, vectorized computations without loops, commonly used in data normalization, feature scaling, and discrete transformation tasks in machine learning pipelines.
"""

# Add 2 to every element in tensor x (broadcasting scalar)
x + 2

# Subtract 2 from every element in tensor x (broadcasting scalar)
x - 2

# Multiply every element in tensor x by 3 (element-wise scalar multiplication)
x * 3

# Divide every element in tensor x by 3 (element-wise scalar division)
x / 3

# Scale all elements by 100, then perform integer division by 3 (truncates decimals)
(x * 100) // 3

# Take the result of the integer division, then compute modulo 2 to get parity (even/odd)
((x * 100) // 3) % 2

"""# Element-wise Tensor Operations with Random Matrices

This code generates two random 2×3 tensors, `a` and `b`, using `torch.rand`, and prints their values to visualize randomly initialized data — commonly used for testing tensor operations, debugging broadcasting behavior, or demonstrating element-wise computations in neural network layers where inputs are sampled from a uniform distribution.
"""

a=torch.rand(2,3)
b=torch.rand(2,3)
print(a)
print(b)

"""# Element-wise Tensor Arithmetic Between Two Tensors

This sequence performs standard element-wise arithmetic operations—addition, subtraction, multiplication, division, and modulo—between two tensors of identical shape, applying each operation pair-wise across corresponding elements; these are foundational for neural network computations such as residual connections, attention scaling, normalization, and gradient updates, where tensor compatibility in shape ensures safe and efficient broadcasting without explicit loops.
"""

a+b
a-b
a*b
a/b
a%b

c=torch.tensor([1,-2,3,-4])

"""# Computing Absolute Values of a Tensor with `torch.abs`

This code computes the element-wise absolute value of tensor `c`, converting all negative values to their positive counterparts while leaving non-negative values unchanged — commonly used to measure magnitude, compute loss functions like L1 norm, or enforce non-negativity in gradient-based optimizations where sign-invariant magnitudes are required.
"""

torch.abs(c)

"""# Negating Tensor Elements with `torch.neg`

This code computes the element-wise negation of tensor `c`, flipping the sign of every value (positive becomes negative, negative becomes positive) — commonly used to reverse gradients, invert transformations, or prepare target signals in loss functions where signed opposition is required, such as in adversarial training or contrastive learning objectives.
"""

torch.neg(c)

d=torch.tensor([1.9,2.3,3.7,4.4])

"""# Rounding Tensor Elements to Nearest Integer with `torch.round`

This code rounds each element of tensor `d` to the nearest integer, preserving the tensor’s shape and dtype (typically converting to float if not already), commonly used to discretize continuous predictions—such as probabilities or outputs from regression layers—into hard class labels or quantized values for post-processing, evaluation, or compatibility with discrete downstream tasks.
"""

torch.round(d)

"""# Rounding Tensor Elements Up to the Nearest Integer with `torch.ceil`

This code computes the ceiling of each element in tensor `d`, rounding every value up to the smallest integer greater than or equal to it — commonly used to enforce minimum thresholds, discretize continuous values upward (e.g., in resource allocation or batch sizing), or prepare data for integer-based indexing and quantization where upward precision is required.
"""

torch.ceil(d)

"""# Rounding Tensor Elements Down to the Nearest Integer with `torch.floor`

This code computes the floor of each element in tensor `d`, rounding every value down to the largest integer less than or equal to it — commonly used to enforce maximum thresholds, discretize continuous values downward (e.g., in binning or quantization), or prepare data for integer-based indexing where truncation toward zero is required for deterministic behavior.
"""

torch.floor(d)

"""# Clamping Tensor Values to a Specified Range with `torch.clamp`

This code restricts all elements of tensor `d` to lie within the range [2, 3], setting any value below 2 to 2 and any value above 3 to 3 — commonly used to prevent extreme activations, stabilize gradients, or enforce physical or logical constraints in neural network outputs, such as pixel intensities, probabilities, or control signals, ensuring robustness and adherence to bounded domains during inference or training.
"""

torch.clamp(d,min=2,max=3)

"""# Generating a Random Integer Tensor with `torch.randint`

This code creates a 2×3 tensor filled with random integers sampled uniformly from the range [0, 10), meaning each element is an integer between 0 (inclusive) and 10 (exclusive) — commonly used to generate discrete labels, mask indices, or synthetic categorical data for testing classification models, embedding layers, or operations that require integer-valued inputs rather than continuous values.
"""

e=torch.randint(size=(2,3),low=0,high=10,dtype=torch.float32)


e

"""# Computing Summations Along Tensor Dimensions with `torch.sum`

This sequence computes the total sum of all elements in tensor `e`, then breaks it down along specific dimensions: summing across columns (`dim=0`) produces a 1D tensor with one value per column (reducing rows), while summing across rows (`dim=1`) produces a 1D tensor with one value per row (reducing columns) — essential for aggregating features, computing loss over batches, or reducing activations in neural networks to obtain summary statistics while preserving structural intent through explicit dimension control.
"""

torch.sum(e)

#sum along columns
torch.sum(e,dim=0)
#sum along rows
torch.sum(e,dim=1)

"""# Computing Mean Values Along Tensor Dimensions with `torch.mean`

This sequence computes the global mean of all elements in tensor `e`, then calculates the mean along specific dimensions: averaging along `dim=0` (columns) produces a row vector where each element is the mean of its corresponding column, effectively reducing the number of rows while preserving column structure — commonly used to compute feature-wise averages across a batch, enabling normalization, statistical analysis, or activation pooling in neural network layers.
"""

#mean
torch.mean(e)
#mean along col
torch.mean(e,dim=0)

"""# Computing the Median of All Elements in a Tensor with `torch.median`

This code computes the median value of all elements in tensor `e`, returning the middle value when all elements are sorted in ascending order — used to obtain a robust central tendency measure that is less sensitive to outliers than the mean, making it useful for anomaly detection, robust statistics in training data, or stabilizing outputs in noisy or skewed distributions within neural network inference.
"""

#median
torch.median(e)

"""# Finding Global Maximum and Minimum Values in a Tensor with `torch.max` and `torch.min`

This sequence retrieves the single largest and smallest values across all elements in tensor `e`, providing the extreme bounds of the data — commonly used to monitor activation ranges, detect saturation or vanishing signals in neural networks, normalize inputs, or set clipping thresholds for stable training and inference.
"""

#max and min
torch.max(e)
torch.min(e)

"""# Computing the Product of All Elements in a Tensor with `torch.prod`

This code calculates the product of all elements in tensor `e`, multiplying every value together to produce a single scalar result — used in probabilistic models for joint likelihood computation, attention scaling, or geometric mean approximations, where cumulative multiplication across features or time steps is required to capture multiplicative interactions in neural network outputs or loss functions.
"""

#product
torch.prod(e)

"""# Computing Standard Deviation of a Tensor with `torch.std`

This code computes the standard deviation of all elements in tensor `e`, measuring the spread or variability of the values around their mean — essential for analyzing activation distributions, detecting training instability, normalizing inputs via standardization, or initializing weights with controlled variance to maintain signal flow in deep neural networks.
"""

#standard deviation
torch.std(e)

"""# Computing Variance of a Tensor with `torch.var`

This code computes the variance of all elements in tensor `e`, measuring the average squared deviation from the mean — used to quantify the spread of activations or weights, monitor training dynamics, and inform initialization strategies (e.g., Xavier/Glorot) that rely on controlled variance to prevent vanishing or exploding gradients in deep neural networks.
"""

#variance
torch.var(e)

"""# Finding the Index of the Maximum Value in a Flattened Tensor with `torch.argmax`

This code returns the single flat index corresponding to the largest element in tensor `e`, treating the tensor as a flattened 1D array — commonly used to identify the position of the dominant activation, such as in classification tasks where the highest score corresponds to the predicted class, or for locating peak values in attention maps, feature responses, or optimization signals during model analysis.
"""

#argmax
torch.argmax(e)

"""# Finding the Index of the Minimum Value in a Flattened Tensor with `torch.argmin`

This code returns the single flat index corresponding to the smallest element in tensor `e`, treating the tensor as a flattened 1D array — commonly used to locate the weakest activation, identify outliers, or find minimal error regions in optimization and attention mechanisms, providing a way to pinpoint critical locations in data without returning the value itself.
"""

#argmin
torch.argmin(e)

f=torch.randint(size=(2,3),low=0,high=10)
g=torch.randint(size=(3,2),low=0,high=10)

print(f)
print(g)

"""# Matrix Multiplication with `torch.matmul`

This code performs standard matrix multiplication between tensors `f` and `g`, computing the dot product of rows from `f` with columns from `g` — the foundational operation in dense neural network layers, where weights are applied to inputs to produce transformed outputs; it enables efficient linear transformations at scale, supporting batched computations and automatic broadcasting for compatibility with convolutional, recurrent, and fully connected architectures.
"""

torch.matmul(f,g)

"""# Computing the Dot Product of Two Vectors with `torch.dot`

This code calculates the dot product (scalar product) of two 1D tensors, `vector1` and `vector2`, by multiplying corresponding elements and summing the results — a fundamental operation in linear algebra used to measure similarity between vectors, compute projections, apply attention weights, or evaluate inner products in neural networks, where the output represents the aligned magnitude of two vectors in shared space.
"""

vector1=torch.tensor([1,2])
vector2=torch.tensor([3,4])

torch.dot(vector1, vector2)

"""# Transposing a Tensor with `torch.transpose`

This code swaps the dimensions of tensor `f` along axes 0 and 1 — converting rows into columns and columns into rows — commonly used to align tensor shapes for matrix multiplication, prepare data for layer inputs (e.g., switching batch and feature dimensions), or match expected formats in operations like attention mechanisms or convolutional layers that require specific axis ordering.
"""

torch.transpose(f, 0,1)

"""# Generating a Random 3×3 Float Tensor with `torch.randint`

This code creates a 3×3 tensor filled with random integers between 0 (inclusive) and 10 (exclusive), then explicitly casts them to `torch.float32` — commonly used to simulate real-valued data with discrete sampling, such as noisy measurements or discretized features, while ensuring compatibility with floating-point operations in neural networks that require single-precision arithmetic for efficiency and numerical stability.
"""

h=torch.randint(size=(3,3),low=0,high=10,dtype=torch.float32)
h

"""# Computing the Determinant of a Square Tensor with `torch.det`

This code calculates the determinant of the 3×3 floating-point tensor `h` — a scalar value that represents the scaling factor of the linear transformation described by the matrix, used to assess invertibility (non-zero determinant = invertible), measure volume change in coordinate transformations, or detect degenerate matrices in optimization and geometric deep learning applications.
"""

torch.det(h)

"""# Computing the Matrix Inverse with `torch.inverse`

This code calculates the inverse of the 3×3 invertible tensor `h`, producing a new matrix that, when multiplied by `h`, yields the identity matrix — essential for solving linear systems, undoing transformations, or computing precision matrices in probabilistic models; it requires that `h` be square and non-singular (determinant ≠ 0), making it critical for stability in neural network operations involving normalization, whitening, or geometric corrections.
"""

torch.inverse(h)













